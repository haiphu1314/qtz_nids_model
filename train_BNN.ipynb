{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usami/miniconda3/envs/human_action/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/usami/miniconda3/envs/human_action/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# modified from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "df_unsw_normal = pd.read_csv('/home/usami/Protocol-Based-Deep-Intrusion-Detection-for-DoS-Normal-and-DDoS-Attacks/normal_ddos_dos_classification/dataset/train/unsw_normal.csv')\n",
    "df_bot_dos = pd.read_csv('/home/usami/Protocol-Based-Deep-Intrusion-Detection-for-DoS-Normal-and-DDoS-Attacks/normal_ddos_dos_classification/dataset/train/bot_iot_dos.csv')\n",
    "df_bot_ddos = pd.read_csv('/home/usami/Protocol-Based-Deep-Intrusion-Detection-for-DoS-Normal-and-DDoS-Attacks/normal_ddos_dos_classification/dataset/train/bot_iot_ddos.csv')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ternary(x: torch.Tensor, delta: float):\n",
    "    return (x >= delta).float() - (x <= -delta).float()\n",
    "\n",
    "\n",
    "class _ternary_without_scale(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs) -> torch.Tensor:\n",
    "        input_f, running_delta, delta, momentum, training = inputs\n",
    "        if momentum > 0:\n",
    "            if training:\n",
    "                ctx.delta = input_f.norm(1).item() * (delta / input_f.numel())  # = delta * |input_f|_1 / n\n",
    "                running_delta.data = momentum * ctx.delta + (1.0 - momentum) * running_delta.data\n",
    "            else:\n",
    "                ctx.delta = running_delta.data.item()\n",
    "        else:\n",
    "            ctx.delta = delta\n",
    "        input_t = _ternary(input_f, ctx.delta)\n",
    "        ctx.save_for_backward(input_f)\n",
    "        return input_t\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        grad_output, = grad_outputs\n",
    "        input_f, = ctx.saved_tensors\n",
    "        grad_input = grad_output * (-1 <= input_f & input_f <= 1).float()\n",
    "        return grad_input, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class _ternary_py(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def ternary_backward(grad_output: torch.Tensor, x: torch.Tensor, delta: float, order: int, threshold: float):\n",
    "        scale = 2 * delta\n",
    "        assert threshold <= scale\n",
    "        tmp = torch.zeros_like(grad_output)\n",
    "        tmp += ((x >= -threshold) & (x <= threshold)).float() * order * \\\n",
    "               (torch.fmod(x / delta + 3, 2) - 1).abs().pow(order - 1)\n",
    "        return grad_output * tmp\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs) -> torch.Tensor:\n",
    "        input_f, running_delta, delta, momentum, training, ctx.order = inputs\n",
    "        if momentum > 0:\n",
    "            if training:\n",
    "                ctx.delta = input_f.norm(1).item() * (delta / input_f.numel())  # = delta * |input_f|_1 / n\n",
    "                running_delta.data = momentum * ctx.delta + (1.0 - momentum) * running_delta.data\n",
    "            else:\n",
    "                ctx.delta = running_delta.data.item()\n",
    "        else:\n",
    "            ctx.delta = delta\n",
    "        input_t = _ternary(input_f, ctx.delta) * (2 * ctx.delta)\n",
    "        ctx.save_for_backward(input_f)\n",
    "        return input_t\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        grad_output, = grad_outputs\n",
    "        input_f, = ctx.saved_tensors\n",
    "        grad_input = _ternary_py.ternary_backward(grad_output, input_f, ctx.delta, ctx.order, 2. * ctx.delta)\n",
    "        return grad_input, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "def ternary(input_f: torch.Tensor, running_delta, delta, momentum, training, order, use_scale=True):\n",
    "    if not use_scale:\n",
    "        return _ternary_without_scale.apply(input_f, running_delta, delta, momentum, training)\n",
    "    else:\n",
    "        return _ternary_py.apply(input_f, running_delta, delta, momentum, training, order)\n",
    "\n",
    "\n",
    "class Ternary(torch.nn.Module):\n",
    "    def __init__(self, config: dict, *arg, **kwargs):\n",
    "        super(Ternary, self).__init__()\n",
    "        self.config = config\n",
    "        self.delta = config.setdefault(\"delta\", 0.5)\n",
    "        self.momentum = config.setdefault(\"momentum\", 0.01)\n",
    "        self.track_running_stats = config.setdefault(\"track_running_stats\", True)\n",
    "        self.order = config.setdefault('order', 2)\n",
    "        self.use_scale = config.setdefault('use_scale', True)\n",
    "        assert self.momentum <= 1 and self.order > 0 and self.delta > 0\n",
    "        self.register_buffer(\"running_delta\", torch.zeros(1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.momentum > 0:\n",
    "            self.running_delta.fill_(self.delta * 0.7979)\n",
    "        else:\n",
    "            self.running_delta.fill_(self.delta)\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        return ternary(input_f, self.running_delta, self.delta, self.momentum,\n",
    "                       self.training and self.track_running_stats, self.order, self.use_scale)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \", \".join([\"{}={}\".format(k, v) for k, v in self.config.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12712/4249798583.py:12: RuntimeWarning: invalid value encountered in log\n",
      "  x[:,i] = np.where(x[:,i] > 0, np.log(x[:,i]), 0)\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([df_unsw_normal, df_bot_dos, df_bot_ddos], ignore_index=True)\n",
    "\n",
    "x = df_combined.loc[:, df_combined.columns != 'category']\n",
    "# x = (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "df_combined.category = pd.factorize(df_combined.category)[0]\n",
    "y = df_combined['category']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "for i in [0,1,2,3,7,8]:\n",
    "    x[:,i] = np.where(x[:,i] > 0, np.log(x[:,i]), 0)\n",
    "    \n",
    "x_label_data = torch.tensor(x, dtype=torch.float).to(device)\n",
    "y_label_data = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "dataset = TensorDataset(x_label_data, y_label_data)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "valid_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8192\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryActivationFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Binarize input to -1 or 1\n",
    "        output = input.sign()\n",
    "        output[output==0] = 1\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.abs() > 1] = 0\n",
    "        return grad_input\n",
    "\n",
    "class BinaryActivation(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return BinaryActivationFunction.apply(input)\n",
    "\n",
    "class QLinear(torch.nn.Linear):\n",
    "    qa_config = {}\n",
    "    qw_config = {}\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        # self.input_quantizer = BinaryActivation()\n",
    "        self.weight_quantizer = BinaryActivation()\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        # input_t = self.input_quantizer(input_f)\n",
    "        weight_b = self.weight_quantizer(self.weight)\n",
    "        out = F.linear(input_f, weight_b, self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Validation Accuracy: 91.39%6\n",
      "Epoch [2/5], Validation Accuracy: 89.58%6\n",
      "Epoch [3/5], Validation Accuracy: 92.36%8\n",
      "Epoch [4/5], Validation Accuracy: 92.36%5\n",
      "Epoch [5/5], Validation Accuracy: 92.36%4\n",
      "Test Accuracy: 92.25%\n"
     ]
    }
   ],
   "source": [
    "class Netb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netb, self).__init__()\n",
    "        self.input_quantizer = BinaryActivation()\n",
    "\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.i1 = nn.Identity()\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        # self.i2 = nn.Identity()\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        # self.i3 = nn.Identity()\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.i2(x)\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.i3(x)\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_b = Netb().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_b.parameters(), 0.01,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model_b.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_b(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model_ter1.set_weights_and_biases()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    model_b.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_b(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final evaluation on the test set\n",
    "model_b.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_b(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model_b.state_dict(), 'saved_best_model_bnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
       "          1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
       "         -1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,\n",
       "         -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
       "         -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
       "         -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,\n",
       "         -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,\n",
       "          1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "         -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
       "          1., -1.],\n",
       "        [-1.,  1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
       "         -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,\n",
       "         -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,\n",
       "         -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
       "          1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "          1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "         -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
       "         -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "          1.,  1.],\n",
       "        [-1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "          1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
       "          1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
       "         -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "         -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
       "          1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
       "          1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "         -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
       "          1.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryActivationFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Binarize input to -1 or 1\n",
    "        output = input.sign()\n",
    "        output[output==0] = 1\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.abs() > 1] = 0\n",
    "        return grad_input\n",
    "\n",
    "class BinaryActivation(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return BinaryActivationFunction.apply(input)\n",
    "\n",
    "class QLinear(torch.nn.Linear):\n",
    "    qa_config = {}\n",
    "    qw_config = {}\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.weight_quantizer = BinaryActivation()\n",
    "    def forward(self, input_f):\n",
    "        # weight_b = self.weight_quantizer(self.weight)\n",
    "        out = F.linear(input_f, self.weight, self.bias)\n",
    "        return out\n",
    "\n",
    "class Netb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netb, self).__init__()\n",
    "        self.input_quantizer = BinaryActivation()\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "\n",
    "model_bnn = Netb().to(device)\n",
    "model_bnn.linear1.state_dict()['weight'].copy_(model_b.linear1.state_dict()['weight'].sign())\n",
    "model_bnn.linear2.state_dict()['weight'].copy_(model_b.linear2.state_dict()['weight'].sign())\n",
    "model_bnn.linear3.state_dict()['weight'].copy_(model_b.linear3.state_dict()['weight'].sign())\n",
    "model_bnn.linear4.state_dict()['weight'].copy_(model_b.linear4.state_dict()['weight'].sign())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.25%\n"
     ]
    }
   ],
   "source": [
    "model_bnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_bnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "# torch.save(model_bnn.state_dict(), 'saved_best_model_bnn_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_concatenate(arr):\n",
    "    # Chuyển đổi -1 thành 1 và 1 thành 0\n",
    "    arr = torch.flip(arr, dims=[0])\n",
    "    converted_arr = [1 if x == -1 else 0 for x in arr]\n",
    "    # Ghép các phần tử lại thành một chuỗi\n",
    "    concatenated_str = ''.join(map(str, converted_arr))\n",
    "\n",
    "    current_length = len(concatenated_str)\n",
    "    total_length = (np.ceil(current_length/32)*32).astype(int)\n",
    "    if current_length < total_length:\n",
    "        padding_length = int(total_length - current_length)\n",
    "        concatenated_str = '0' * padding_length + concatenated_str\n",
    "    # Chuyển chuỗi thành số nhị phân và sau đó thành số hex\n",
    "    hex_value = hex(int(concatenated_str, 2))[2:]  # Bỏ tiền tố '0x'\n",
    "\n",
    "    # Đảm bảo chuỗi hex đủ độ dài cần thiết\n",
    "    hex_length = total_length // 4  # 1 hex digit = 4 bits\n",
    "    if len(hex_value) < hex_length:\n",
    "        hex_value = '0' * (hex_length - len(hex_value)) + hex_value\n",
    "    return hex_value\n",
    "\n",
    "def save_model_parameters_to_txt(model, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            f.write(f'{name}\\n')\n",
    "            f.write(f'input_channel: {param.shape[1]}\\n')\n",
    "            f.write(f'output_channel: {param.shape[0]}\\n')\n",
    "            for param_e in param:\n",
    "                hex_conv = convert_and_concatenate(param_e)\n",
    "                segments = [hex_conv[i:i+8] for i in range(0, len(hex_conv), 8)]\n",
    "                segments.reverse()\n",
    "                for segment in segments:\n",
    "                    f.write(f'0x{segment}\\n')\n",
    "            f.write(f'\\n')\n",
    "    return 0\n",
    "\n",
    "# Gọi hàm để lưu thông số mô hình\n",
    "a = save_model_parameters_to_txt(model_bnn, 'model_parameters.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Validation Accuracy: 91.52%1\n",
      "Epoch [2/2], Validation Accuracy: 91.63%0\n",
      "Test Accuracy: 91.51%\n"
     ]
    }
   ],
   "source": [
    "class Net_fp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_fp, self).__init__()\n",
    "        self.linear1 = nn.Linear(13, 128, bias = False)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear2 = nn.Linear(128, 128, bias = False)\n",
    "        # self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.linear3 = nn.Linear(128, 128, bias = False)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.linear4 = nn.Linear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        # x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_fp = Net_fp().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_fp.parameters(), 0.01,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model_fp.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_fp(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model_ter1.set_weights_and_biases()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    model_fp.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_fp(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final evaluation on the test set\n",
    "model_fp.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_fp(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model_fp.state_dict(), 'saved_best_model_fp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_fp_parameters_to_txt(model, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            f.write(f'{name}\\n')\n",
    "            if('linear' in name and 'weight'in name):\n",
    "                f.write(f'input_channel: {param.shape[1]}\\n')\n",
    "                f.write(f'output_channel: {param.shape[0]}\\n')\n",
    "            f.write(f'{param}\\n')\n",
    "            f.write(f'\\n')\n",
    "    return 0\n",
    "\n",
    "# Gọi hàm để lưu thông số mô hình\n",
    "a = save_model_fp_parameters_to_txt(model_fp, 'fp_model_parameters.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ternary(x: torch.Tensor, delta: float):\n",
    "    return (x >= delta).float() - (x <= -delta).float()\n",
    "\n",
    "class _ternary_py(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def ternary_backward(grad_output: torch.Tensor, x: torch.Tensor, delta: float, order: int, threshold: float):\n",
    "        scale = 2 * delta\n",
    "        assert threshold <= scale\n",
    "        tmp = torch.zeros_like(grad_output)\n",
    "        tmp += ((x >= -threshold) & (x <= threshold)).float() * order * \\\n",
    "               (torch.fmod(x / delta + 3, 2) - 1).abs().pow(order - 1)\n",
    "        return grad_output * tmp\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs) -> torch.Tensor:\n",
    "        input_f, running_delta, delta, momentum, training, ctx.order = inputs\n",
    "        if momentum > 0:\n",
    "            if training:\n",
    "                ctx.delta = input_f.norm(1).item() * (delta / input_f.numel())  # = delta * |input_f|_1 / n\n",
    "                running_delta.data = momentum * ctx.delta + (1.0 - momentum) * running_delta.data\n",
    "            else:\n",
    "                ctx.delta = running_delta.data.item()\n",
    "        else:\n",
    "            ctx.delta = delta\n",
    "        # input_t = _ternary(input_f, ctx.delta) * (2 * ctx.delta)\n",
    "        input_t = _ternary(input_f, ctx.delta)\n",
    "        ctx.save_for_backward(input_f)\n",
    "        return input_t\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        grad_output, = grad_outputs\n",
    "        input_f, = ctx.saved_tensors\n",
    "        grad_input = _ternary_py.ternary_backward(grad_output, input_f, ctx.delta, ctx.order, 2. * ctx.delta)\n",
    "        return grad_input, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "def ternary(input_f: torch.Tensor, running_delta, delta, momentum, training, order):\n",
    "    return _ternary_py.apply(input_f, running_delta, delta, momentum, training, order)\n",
    "\n",
    "\n",
    "class Ternary(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Ternary, self).__init__()\n",
    "        config = {}\n",
    "        self.config = config\n",
    "        self.delta = config.setdefault(\"delta\", 0.5)\n",
    "        self.momentum = config.setdefault(\"momentum\", 0.01)\n",
    "        self.track_running_stats = config.setdefault(\"track_running_stats\", True)\n",
    "        self.order = config.setdefault('order', 2)\n",
    "        # self.use_scale = config.setdefault('use_scale', True)\n",
    "        assert self.momentum <= 1 and self.order > 0 and self.delta > 0\n",
    "        self.register_buffer(\"running_delta\", torch.zeros(1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.momentum > 0:\n",
    "            self.running_delta.fill_(self.delta * 0.7979)\n",
    "        else:\n",
    "            self.running_delta.fill_(self.delta)\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        return ternary(input_f, self.running_delta, self.delta, self.momentum,\n",
    "                       self.training and self.track_running_stats, self.order)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \", \".join([\"{}={}\".format(k, v) for k, v in self.config.items()])\n",
    "class QLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.input_quantizer = Ternary()\n",
    "        self.weight_quantizer = Ternary()\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        input_t = self.input_quantizer(input_f)\n",
    "        weight_b = self.weight_quantizer(self.weight)\n",
    "        out = F.linear(input_t, weight_b, self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Validation Accuracy: 84.68%2\n",
      "Epoch [2/4], Validation Accuracy: 91.85%8\n",
      "Epoch [3/4], Validation Accuracy: 91.85%4\n",
      "Epoch [4/4], Validation Accuracy: 91.85%9\n",
      "Test Accuracy: 91.74%\n"
     ]
    }
   ],
   "source": [
    "class Net_tnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_tnn, self).__init__()\n",
    "\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.i1 = nn.Identity()\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "        # self.bn2 = nn.BatchNorm1d(128)\n",
    "        # self.i2 = nn.Identity()\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        # self.i3 = nn.Identity()\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "        # x = self.bn1(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.bn2(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "        # x = self.bn3(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_tnn = Net_tnn().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_tnn.parameters(), 0.01,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model_tnn.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_tnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model_ter1.set_weights_and_biases()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    model_tnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_tnn(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final evaluation on the test set\n",
    "model_tnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_tnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model_tnn.state_dict(), 'saved_best_model_tnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear2.weight\n",
      "linear3.weight\n",
      "linear4.weight\n"
     ]
    }
   ],
   "source": [
    "def convert_and_concatenate_tnn(arr):\n",
    "    arr = torch.flip(arr, dims=[0])\n",
    "    # Chuyển đổi -1 thành 1 và 1 thành 0\n",
    "    converted_bit0_arr = [1 if x == -1 else 0 for x in arr]\n",
    "    converted_bit1_arr = [1 if x == 1 else 0 for x in arr]\n",
    "    concatenated_bit0_str = ''.join(map(str, converted_bit0_arr))\n",
    "    concatenated_bit1_str = ''.join(map(str, converted_bit1_arr))\n",
    "\n",
    "\n",
    "    current_length = np.max([len(concatenated_bit0_str),len(concatenated_bit0_str)])\n",
    "    total_length = (np.ceil(current_length/32)*32).astype(int)\n",
    "    if current_length < total_length:\n",
    "        padding_length0 = int(total_length - len(concatenated_bit0_str))\n",
    "        padding_length1 = int(total_length - len(concatenated_bit1_str))\n",
    "        concatenated_bit0_str = '0' * padding_length0 + concatenated_bit0_str\n",
    "        concatenated_bit1_str = '0' * padding_length1 + concatenated_bit1_str\n",
    "\n",
    "    # Chuyển chuỗi thành số nhị phân và sau đó thành số hex\n",
    "    hex_value_bit0 = hex(int(concatenated_bit0_str, 2))[2:]  # Bỏ tiền tố '0x'\n",
    "    hex_value_bit1 = hex(int(concatenated_bit1_str, 2))[2:]  # Bỏ tiền tố '0x'\n",
    "    # Đảm bảo chuỗi hex đủ độ dài cần thiết\n",
    "    if(len(hex_value_bit0)>len(hex_value_bit1)):\n",
    "        hex_value_bit1 = '0' * (len(hex_value_bit0)-len(hex_value_bit1)) + hex_value_bit1\n",
    "    elif(len(hex_value_bit0)<len(hex_value_bit1)):\n",
    "        hex_value_bit0 = '0' * (len(hex_value_bit1)-len(hex_value_bit0)) + hex_value_bit0\n",
    "\n",
    "    hex_length = total_length // 4  # 1 hex digit = 4 bits\n",
    "    if len(hex_value_bit0) < hex_length:\n",
    "        hex_value_bit0 = '0' * (hex_length - len(hex_value_bit0)) + hex_value_bit0\n",
    "        hex_value_bit1 = '0' * (hex_length - len(hex_value_bit1)) + hex_value_bit1\n",
    "    return hex_value_bit0, hex_value_bit1\n",
    "\n",
    "def save_tnn_model_parameters_to_txt(model, file_path):\n",
    "    input_thres = []\n",
    "    weight_thres = []\n",
    "    for name, module in model.named_modules():        \n",
    "        if 'input_quantizer' in name:\n",
    "            input_thres = np.append(input_thres, module.running_delta.item())\n",
    "        if 'weight_quantizer' in name:\n",
    "            weight_thres = np.append(weight_thres, module.running_delta.item())\n",
    "    # input_thres = np.append(input_thres, 0)\n",
    "    cnt=0\n",
    "    with open(file_path, 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            f.write(f'{name}\\n')\n",
    "            f.write(f'input_channel: {param.shape[1]}\\n')\n",
    "            f.write(f'output_channel: {param.shape[0]}\\n')\n",
    "            f.write(f'input_thres: {input_thres[cnt]}\\n')\n",
    "\n",
    "            param = _ternary(param, weight_thres[cnt])\n",
    "            print(name)\n",
    "            for param_e in param:\n",
    "                hex_conv_bit0, hex_conv_bit1 = convert_and_concatenate_tnn(param_e)\n",
    "                segments0 = [hex_conv_bit0[i:i+8] for i in range(0, len(hex_conv_bit0), 8)]\n",
    "                segments1 = [hex_conv_bit1[i:i+8] for i in range(0, len(hex_conv_bit1), 8)]\n",
    "                segments0.reverse()\n",
    "                segments1.reverse()\n",
    "                for j in range (len(segments0)):\n",
    "                    f.write(f'0x{segments0[j]}\\n')\n",
    "                    f.write(f'0x{segments1[j]}\\n')\n",
    "            f.write(f'\\n')\n",
    "            cnt+=1\n",
    "    return 0\n",
    "\n",
    "# Gọi hàm để lưu thông số mô hình\n",
    "a = save_tnn_model_parameters_to_txt(model_tnn, 'tnn_model_parameters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net_tnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_tnn, self).__init__()\n",
    "\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.i1 = nn.Identity()\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "        # self.bn2 = nn.BatchNorm1d(128)\n",
    "        # self.i2 = nn.Identity()\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        # self.i3 = nn.Identity()\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "        print(\"linear1: \")\n",
    "        print(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "        print(\"linear2: \")\n",
    "        print(x)\n",
    "        # x = self.bn2(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "        print(\"linear3: \")\n",
    "        print(x)\n",
    "        # x = self.bn3(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        print(\"linear4: \")\n",
    "        print(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "model_tnn_test = Net_tnn().to(device)\n",
    "checkpoint = torch.load('saved_best_model_tnn.pt')\n",
    "model_tnn_test.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1: \n",
      "tensor([[-3.,  0.,  3.,  2., -4.,  0.,  1.,  1.,  0.,  1., -3., -1.,  0.,  1.,\n",
      "          1.,  1., -3.,  1.,  1.,  1.,  3.,  0., -3.,  1.,  1., -1., -1.,  0.,\n",
      "          0., -1., -1.,  1., -1.,  5., -2.,  1., -3., -2.,  1.,  1., -1.,  4.,\n",
      "          1., -2., -1., -2.,  1.,  2.,  1., -2.,  1.,  0.,  0.,  1.,  1., -1.,\n",
      "          0.,  2.,  0., -1.,  2.,  2.,  0., -1., -2., -3., -3., -3., -3.,  1.,\n",
      "         -4.,  1., -3., -2., -1.,  1.,  0.,  3., -1.,  1.,  2.,  0., -2., -3.,\n",
      "          0.,  2., -1.,  1., -2., -2.,  2.,  2., -1., -2.,  1.,  0.,  2.,  4.,\n",
      "          5., -3., -2., -1.,  1., -1., -4.,  0.,  2., -2.,  1., -1.,  0., -1.,\n",
      "          0.,  1.,  3.,  1.,  1.,  3.,  3., -1.,  1.,  2.,  3., -2.,  0.,  2.,\n",
      "          0., -2.]], device='cuda:0')\n",
      "linear2: \n",
      "tensor([[ 12.,   6., -10., -14.,  -5.,   8.,  10.,   1.,  31.,  12.,  10.,  -5.,\n",
      "          -2.,  11., -12., -24.,  -1.,  -2.,  11.,  -4.,  12.,   4.,   2., -14.,\n",
      "          -5.,  -2.,  -3.,   1.,   7., -15.,   5.,  -4.,  -4.,   9.,   5.,   2.,\n",
      "           8., -11.,  -5., -10.,  -2.,   5.,   5.,  -5., -17.,   3.,  11.,  -4.,\n",
      "         -11.,   3., -10.,   9.,   1.,  -5.,  14.,  -3.,  -5.,   0.,  24.,   4.,\n",
      "           6.,  14.,   9.,  -2.,   7.,   0.,  11.,   0.,   3.,  22., -17., -16.,\n",
      "          14.,   8., -14.,  -2., -12.,  -8.,   3., -13., -23.,   5., -15.,  -2.,\n",
      "           6.,  14., -13.,  27.,   3.,   8., -13.,   6.,  10.,  -7.,   3.,  11.,\n",
      "          -5., -18.,   1.,   9.,  12., -11., -10.,  -8.,  -3., -18., -11.,  -7.,\n",
      "          -8.,   8.,   8.,  16., -13.,  -2.,   8., -18.,  -8.,  -3.,  -2., -19.,\n",
      "          11.,  -2.,  13., -21., -10.,  -4.,  11.,  15.]], device='cuda:0')\n",
      "linear3: \n",
      "tensor([[  1., -11.,  13.,  -5.,   0.,  10.,   6.,   5.,  -1.,  -8.,  -1.,   0.,\n",
      "           0.,  -6.,   1.,  -1.,   0.,   2.,  -7.,   2.,  -7.,  10.,  19.,   1.,\n",
      "           5., -16.,  -1.,  -4.,   3., -20.,  -1.,   8.,  17.,  11.,   2.,  13.,\n",
      "           3.,  -5.,   3.,   6.,   2.,  -1.,  -4.,   3.,  12.,  14.,  16.,  -4.,\n",
      "          -2.,  -4., -10.,  14.,   7.,  -2.,  -4.,   4.,   5.,   2.,  13.,  11.,\n",
      "          -2.,  -5.,   6.,  -6.,  11.,  -7.,  -8.,  12.,  14.,   3.,  -6.,  12.,\n",
      "          16., -12.,   3.,   0.,  -1.,  -4., -15.,  -4.,  10.,  -5., -12.,  13.,\n",
      "           1.,   0.,  -2.,   3., -10.,   5.,   7.,  16.,   9.,  12.,  -1.,   9.,\n",
      "         -15.,  18.,   1.,  -9.,   0.,   7.,  -4.,   9.,  -5.,   3.,  -1., -12.,\n",
      "           1.,  11., -18.,  -8.,   5.,   4.,   1.,  12.,  -3.,  11.,  -7.,  -1.,\n",
      "           8.,   1.,  -6.,  -5.,  12.,  -5., -19.,   1.]], device='cuda:0')\n",
      "linear4: \n",
      "tensor([[-18.,  -9.,  17.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on the test set\n",
    "model_tnn_test.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test = torch.tensor([[0.0000,  0.0000,  0.0000,  0.0000, -0.6556,  0.9844,  0.2236, -1.4968, -1.4968, -0.9711, -0.1279,  0.2186,  0.2208]]).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model_tnn_test(test)\n",
    "\n",
    "# test_accuracy = 100 * correct / total\n",
    "# print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3157"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TBN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.input_quantizer = Ternary()\n",
    "        self.weight_quantizer = BinaryActivation()\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        input_t = self.input_quantizer(input_f)\n",
    "        weight_b = self.weight_quantizer(self.weight)\n",
    "        out = F.linear(input_t, weight_b, self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Validation Accuracy: 86.25%1\n",
      "Epoch [2/4], Validation Accuracy: 88.99%8\n",
      "Epoch [3/4], Validation Accuracy: 92.04%7\n",
      "Epoch [4/4], Validation Accuracy: 92.04%3\n",
      "Test Accuracy: 91.98%\n"
     ]
    }
   ],
   "source": [
    "class Net_tbn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_tbn, self).__init__()\n",
    "\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.i1 = nn.Identity()\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "        # self.bn2 = nn.BatchNorm1d(128)\n",
    "        # self.i2 = nn.Identity()\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        # self.i3 = nn.Identity()\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "        # x = self.bn1(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.bn2(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "        # x = self.bn3(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_tbn = Net_tbn().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_tbn.parameters(), 0.01,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model_tbn.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_tbn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model_ter1.set_weights_and_biases()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    model_tbn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_tbn(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final evaluation on the test set\n",
    "model_tbn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_tbn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model_tbn.state_dict(), 'saved_best_model_tbn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BinaryActivation' object has no attribute 'running_delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[681], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Gọi hàm để lưu thông số mô hình\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43msave_tbn_model_parameters_to_txt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_tbn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtbn_model_parameters.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[681], line 32\u001b[0m, in \u001b[0;36msave_tbn_model_parameters_to_txt\u001b[0;34m(model, file_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m         input_thres \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(input_thres, module\u001b[38;5;241m.\u001b[39mrunning_delta\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_quantizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m---> 32\u001b[0m         weight_thres \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(weight_thres, \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_delta\u001b[49m\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# input_thres = np.append(input_thres, 0)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m cnt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/human_action/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BinaryActivation' object has no attribute 'running_delta'"
     ]
    }
   ],
   "source": [
    "def _binary(x: torch.Tensor):\n",
    "    return (x >= 0).float() - (x < 0).float()\n",
    "\n",
    "def convert_and_concatenate_tbn(arr):\n",
    "    # Chuyển đổi -1 thành 1 và 1 thành 0\n",
    "    arr = torch.flip(arr, dims=[0])\n",
    "    converted_arr = [1 if x == -1 else 0 for x in arr]\n",
    "    # Ghép các phần tử lại thành một chuỗi\n",
    "    concatenated_str = ''.join(map(str, converted_arr))\n",
    "\n",
    "    current_length = len(concatenated_str)\n",
    "    total_length = (np.ceil(current_length/32)*32).astype(int)\n",
    "    if current_length < total_length:\n",
    "        padding_length = int(total_length - current_length)\n",
    "        concatenated_str = '0' * padding_length + concatenated_str\n",
    "    # Chuyển chuỗi thành số nhị phân và sau đó thành số hex\n",
    "    hex_value = hex(int(concatenated_str, 2))[2:]  # Bỏ tiền tố '0x'\n",
    "\n",
    "    # Đảm bảo chuỗi hex đủ độ dài cần thiết\n",
    "    hex_length = total_length // 4  # 1 hex digit = 4 bits\n",
    "    if len(hex_value) < hex_length:\n",
    "        hex_value = '0' * (hex_length - len(hex_value)) + hex_value\n",
    "    return hex_value\n",
    "\n",
    "def save_tbn_model_parameters_to_txt(model, file_path):\n",
    "    input_thres = []\n",
    "    for name, module in model.named_modules():        \n",
    "        if 'input_quantizer' in name:\n",
    "            input_thres = np.append(input_thres, module.running_delta.item())\n",
    "    # input_thres = np.append(input_thres, 0)\n",
    "    cnt=0\n",
    "    with open(file_path, 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            f.write(f'{name}\\n')\n",
    "            f.write(f'input_channel: {param.shape[1]}\\n')\n",
    "            f.write(f'output_channel: {param.shape[0]}\\n')\n",
    "            f.write(f'input_thres: {input_thres[cnt]}\\n')\n",
    "\n",
    "            param = _binary(param)\n",
    "            print(name)\n",
    "            for param_e in param:\n",
    "                hex_conv_bit = convert_and_concatenate_tbn(param_e)\n",
    "                segments = [hex_conv_bit[i:i+8] for i in range(0, len(hex_conv_bit), 8)]\n",
    "                segments.reverse()\n",
    "                for j in range (len(segments)):\n",
    "                    f.write(f'0x{segments[j]}\\n')\n",
    "            f.write(f'\\n')\n",
    "            cnt+=1\n",
    "    return 0\n",
    "\n",
    "# Gọi hàm để lưu thông số mô hình\n",
    "a = save_tbn_model_parameters_to_txt(model_tbn, 'tbn_model_parameters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
       "          1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
       "         -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "          1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
       "          1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
       "         -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
       "         -1., -1.],\n",
       "        [-1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
       "          1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1.,\n",
       "          1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
       "         -1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,\n",
       "         -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
       "         -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,\n",
       "          1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
       "         -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
       "         -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,\n",
       "         -1.,  1.],\n",
       "        [ 1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
       "          1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
       "          1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "         -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
       "          1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
       "          1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
       "         -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "          1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
       "         -1.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.input_quantizer = Ternary()\n",
    "        self.weight_quantizer = BinaryActivation()\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        input_t = self.input_quantizer(input_f)\n",
    "        weight_b = self.weight_quantizer(self.weight)\n",
    "        out = F.linear(input_t, weight_b, self.bias)\n",
    "        return out\n",
    "\n",
    "class Net_tbn_test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_tbn_test, self).__init__()\n",
    "\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.i1 = nn.Identity()\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "        # self.bn2 = nn.BatchNorm1d(128)\n",
    "        # self.i2 = nn.Identity()\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        # self.i3 = nn.Identity()\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "        print(\"linear1: \")\n",
    "        print(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "        print(\"linear2: \")\n",
    "        print(x)\n",
    "        # x = self.bn2(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "        print(\"linear3: \")\n",
    "        print(x)\n",
    "        # x = self.bn3(x)\n",
    "\n",
    "        # x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        print(\"linear4: \")\n",
    "        print(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "model_tbn_test = Net_tbn_test().to(device)\n",
    "checkpoint = torch.load('saved_best_model_tbn.pt')\n",
    "model_tbn_test.load_state_dict(checkpoint)\n",
    "model_tbn_test.linear1.state_dict()['weight'].copy_(_binary(model_tbn_test.linear1.state_dict()['weight']))\n",
    "model_tbn_test.linear2.state_dict()['weight'].copy_(_binary(model_tbn_test.linear2.state_dict()['weight']))\n",
    "model_tbn_test.linear3.state_dict()['weight'].copy_(_binary(model_tbn_test.linear3.state_dict()['weight']))\n",
    "model_tbn_test.linear4.state_dict()['weight'].copy_(_binary(model_tbn_test.linear4.state_dict()['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1: \n",
      "tensor([[-1., -3.,  1., -1.,  3., -1., -3.,  3.,  3.,  1.,  1., -3.,  3., -3.,\n",
      "          3.,  1.,  1.,  3.,  3., -1., -1., -1., -1.,  1., -3.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1., -1.,  3.,  5., -5., -1.,  3.,  1., -1., -1.,  5., -5.,\n",
      "          1., -5.,  1., -3., -1., -3.,  5.,  1., -1., -1., -1.,  1., -3., -5.,\n",
      "          1., -3., -3., -1.,  5., -3., -5.,  1., -1.,  3., -1., -1., -3.,  3.,\n",
      "         -1.,  5., -1., -1., -5., -1., -3.,  1.,  5., -1.,  5.,  3.,  3., -1.,\n",
      "          1.,  3., -1., -3.,  1.,  5.,  5.,  1.,  3.,  3.,  1.,  3.,  1.,  3.,\n",
      "         -1., -1.,  1., -3.,  3., -5., -1.,  1., -3., -1.,  1.,  1.,  5.,  1.,\n",
      "         -3.,  5.,  3., -1., -5., -1., -3., -1.,  5.,  3., -1., -1.,  1.,  1.,\n",
      "         -3.,  1.]], device='cuda:0')\n",
      "linear2: \n",
      "tensor([[ -4., -28., -30.,  -2.,   4.,  48.,  -2., -14.,  -4., -16.,   8.,  16.,\n",
      "           6.,   4.,   8.,  28.,  -8., -16.,   6.,  -6.,  20.,  26.,   4., -30.,\n",
      "          20., -10., -16.,   6.,   4., -42.,  18., -34., -10., -12.,   2.,   4.,\n",
      "           2., -18.,  18., -16.,  14.,   2.,  28.,  18.,  -8.,   6., -16.,  18.,\n",
      "          20.,  22., -10.,  12., -16.,   4.,  14., -10., -16.,  -8., -34.,  -8.,\n",
      "          -2.,  -2.,  -6.,  28.,   6.,  -8.,  -4.,  26.,  20.,  28.,  28.,  12.,\n",
      "          14.,   6.,  12., -10.,  18., -10., -12., -16.,   0., -42., -24.,  16.,\n",
      "          12., -12.,  26.,  12.,  22.,  10.,  22., -28., -24.,  -4.,  18.,  22.,\n",
      "         -10.,   2., -22., -34.,  10.,  18.,   4., -32.,   2.,   6., -12., -12.,\n",
      "         -20.,   0.,  -4., -10.,  22.,  32.,   8., -20., -22., -22., -20.,   6.,\n",
      "          24.,  10.,   4., -22., -28., -28.,  24.,  18.]], device='cuda:0')\n",
      "linear3: \n",
      "tensor([[ -2.,   0.,  -2.,  18.,   2.,   2.,   4.,   0.,  -4.,  10., -32.,  -6.,\n",
      "         -18.,  -2.,   0., -10.,   2., -10., -28., -10.,  18.,  24.,   4., -12.,\n",
      "          12.,   0.,  16.,   6.,  -4.,  -2.,  18.,   6.,  -8.,  10.,  -4.,  18.,\n",
      "          10.,  -8.,   4.,  -8.,  -6., -14.,  10.,   6.,   2.,  20.,   4.,   4.,\n",
      "          22.,  24.,  -6.,  16., -10., -14.,   0.,   2.,   0.,   0.,  22., -30.,\n",
      "          -8.,   2., -18.,   2.,  -2.,  16.,  -8.,  -6., -22.,  28.,   8.,  12.,\n",
      "          20.,   6.,  -4., -16., -14.,  -4.,  -6.,  20.,   8.,  12., -10.,  12.,\n",
      "          12.,  16., -18., -14., -24.,  -4., -10.,   6.,   8., -14.,  10.,   0.,\n",
      "          -4.,   6., -18.,  -4., -12.,  -2.,  -2.,  -8., -20.,   0.,   6.,   6.,\n",
      "           8.,  -8.,  16.,   4.,   4.,   6., -12.,   8.,  -2.,   2.,  -4.,   6.,\n",
      "          16., -14.,   2., -28.,  -8.,  -8., -12.,  -2.]], device='cuda:0')\n",
      "linear4: \n",
      "tensor([[ -1., -17.,  45.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on the test set\n",
    "model_tbn_test.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test = torch.tensor([[0.0000,  0.0000,  0.0000,  0.0000, -0.6556,  0.9844,  0.2236, -1.4968, -1.4968, -0.9711, -0.1279,  0.2186,  0.2208]]).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model_tbn_test(test)\n",
    "\n",
    "# test_accuracy = 100 * correct / total\n",
    "# print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.],\n",
       "        [ 1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
       "        [ 1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.],\n",
       "        [ 1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [ 1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.],\n",
       "        [ 1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.],\n",
       "        [ 1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1.],\n",
       "        [ 1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.],\n",
       "        [-1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.],\n",
       "        [-1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.],\n",
       "        [-1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
       "        [ 1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [-1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.],\n",
       "        [-1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.],\n",
       "        [-1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.],\n",
       "        [ 1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1.],\n",
       "        [ 1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.],\n",
       "        [-1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.],\n",
       "        [-1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.],\n",
       "        [ 1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.],\n",
       "        [ 1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.],\n",
       "        [ 1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.],\n",
       "        [ 1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.],\n",
       "        [-1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.],\n",
       "        [-1., -1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.],\n",
       "        [ 1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1.],\n",
       "        [ 1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.],\n",
       "        [-1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
       "        [-1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.],\n",
       "        [-1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.],\n",
       "        [ 1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.],\n",
       "        [-1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.],\n",
       "        [-1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.],\n",
       "        [ 1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.],\n",
       "        [ 1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.],\n",
       "        [-1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.],\n",
       "        [-1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.],\n",
       "        [ 1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
       "        [ 1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1.],\n",
       "        [ 1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.],\n",
       "        [-1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.],\n",
       "        [ 1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.],\n",
       "        [ 1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.],\n",
       "        [ 1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.],\n",
       "        [ 1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1.],\n",
       "        [-1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.],\n",
       "        [ 1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.],\n",
       "        [-1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
       "        [-1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.],\n",
       "        [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.],\n",
       "        [-1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.],\n",
       "        [ 1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1.],\n",
       "        [-1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.],\n",
       "        [-1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.],\n",
       "        [-1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.],\n",
       "        [ 1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.],\n",
       "        [-1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.],\n",
       "        [-1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.],\n",
       "        [ 1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.],\n",
       "        [-1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.],\n",
       "        [-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.],\n",
       "        [-1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.],\n",
       "        [ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.],\n",
       "        [ 1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
       "        [-1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.],\n",
       "        [-1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.],\n",
       "        [ 1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
       "        [ 1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
       "        [ 1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.],\n",
       "        [ 1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.],\n",
       "        [-1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.],\n",
       "        [-1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.],\n",
       "        [-1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.],\n",
       "        [-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
       "        [ 1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.],\n",
       "        [-1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
       "        [-1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.],\n",
       "        [-1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.],\n",
       "        [-1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.],\n",
       "        [ 1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.],\n",
       "        [ 1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.],\n",
       "        [ 1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.],\n",
       "        [ 1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.],\n",
       "        [ 1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
       "        [-1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.],\n",
       "        [-1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.],\n",
       "        [-1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.],\n",
       "        [ 1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.],\n",
       "        [-1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.],\n",
       "        [-1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
       "        [ 1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.],\n",
       "        [-1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.],\n",
       "        [-1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.],\n",
       "        [-1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.],\n",
       "        [ 1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.],\n",
       "        [ 1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
       "        [ 1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.],\n",
       "        [ 1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.],\n",
       "        [ 1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.],\n",
       "        [ 1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.],\n",
       "        [-1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.],\n",
       "        [ 1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.],\n",
       "        [-1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.],\n",
       "        [ 1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.],\n",
       "        [-1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.],\n",
       "        [ 1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.],\n",
       "        [-1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.],\n",
       "        [-1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.],\n",
       "        [-1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.],\n",
       "        [ 1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.],\n",
       "        [-1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],\n",
       "        [-1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_binary(model_tbn_test.linear1.state_dict()['weight'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human_action",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
