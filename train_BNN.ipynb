{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usami/miniconda3/envs/human_action/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/usami/miniconda3/envs/human_action/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# modified from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "df_unsw_normal = pd.read_csv('/home/usami/Protocol-Based-Deep-Intrusion-Detection-for-DoS-Normal-and-DDoS-Attacks/normal_ddos_dos_classification/dataset/train/unsw_normal.csv')\n",
    "df_bot_dos = pd.read_csv('/home/usami/Protocol-Based-Deep-Intrusion-Detection-for-DoS-Normal-and-DDoS-Attacks/normal_ddos_dos_classification/dataset/train/bot_iot_dos.csv')\n",
    "df_bot_ddos = pd.read_csv('/home/usami/Protocol-Based-Deep-Intrusion-Detection-for-DoS-Normal-and-DDoS-Attacks/normal_ddos_dos_classification/dataset/train/bot_iot_ddos.csv')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ternary(x: torch.Tensor, delta: float):\n",
    "    return (x >= delta).float() - (x <= -delta).float()\n",
    "\n",
    "\n",
    "class _ternary_without_scale(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs) -> torch.Tensor:\n",
    "        input_f, running_delta, delta, momentum, training = inputs\n",
    "        if momentum > 0:\n",
    "            if training:\n",
    "                ctx.delta = input_f.norm(1).item() * (delta / input_f.numel())  # = delta * |input_f|_1 / n\n",
    "                running_delta.data = momentum * ctx.delta + (1.0 - momentum) * running_delta.data\n",
    "            else:\n",
    "                ctx.delta = running_delta.data.item()\n",
    "        else:\n",
    "            ctx.delta = delta\n",
    "        input_t = _ternary(input_f, ctx.delta)\n",
    "        ctx.save_for_backward(input_f)\n",
    "        return input_t\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        grad_output, = grad_outputs\n",
    "        input_f, = ctx.saved_tensors\n",
    "        grad_input = grad_output * (-1 <= input_f & input_f <= 1).float()\n",
    "        return grad_input, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class _ternary_py(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def ternary_backward(grad_output: torch.Tensor, x: torch.Tensor, delta: float, order: int, threshold: float):\n",
    "        scale = 2 * delta\n",
    "        assert threshold <= scale\n",
    "        tmp = torch.zeros_like(grad_output)\n",
    "        tmp += ((x >= -threshold) & (x <= threshold)).float() * order * \\\n",
    "               (torch.fmod(x / delta + 3, 2) - 1).abs().pow(order - 1)\n",
    "        return grad_output * tmp\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, *inputs) -> torch.Tensor:\n",
    "        input_f, running_delta, delta, momentum, training, ctx.order = inputs\n",
    "        if momentum > 0:\n",
    "            if training:\n",
    "                ctx.delta = input_f.norm(1).item() * (delta / input_f.numel())  # = delta * |input_f|_1 / n\n",
    "                running_delta.data = momentum * ctx.delta + (1.0 - momentum) * running_delta.data\n",
    "            else:\n",
    "                ctx.delta = running_delta.data.item()\n",
    "        else:\n",
    "            ctx.delta = delta\n",
    "        input_t = _ternary(input_f, ctx.delta) * (2 * ctx.delta)\n",
    "        ctx.save_for_backward(input_f)\n",
    "        return input_t\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        grad_output, = grad_outputs\n",
    "        input_f, = ctx.saved_tensors\n",
    "        grad_input = _ternary_py.ternary_backward(grad_output, input_f, ctx.delta, ctx.order, 2. * ctx.delta)\n",
    "        return grad_input, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "def ternary(input_f: torch.Tensor, running_delta, delta, momentum, training, order, use_scale=True):\n",
    "    if not use_scale:\n",
    "        return _ternary_without_scale.apply(input_f, running_delta, delta, momentum, training)\n",
    "    else:\n",
    "        return _ternary_py.apply(input_f, running_delta, delta, momentum, training, order)\n",
    "\n",
    "\n",
    "class Ternary(torch.nn.Module):\n",
    "    def __init__(self, config: dict, *arg, **kwargs):\n",
    "        super(Ternary, self).__init__()\n",
    "        self.config = config\n",
    "        self.delta = config.setdefault(\"delta\", 0.5)\n",
    "        self.momentum = config.setdefault(\"momentum\", 0.01)\n",
    "        self.track_running_stats = config.setdefault(\"track_running_stats\", True)\n",
    "        self.order = config.setdefault('order', 2)\n",
    "        self.use_scale = config.setdefault('use_scale', True)\n",
    "        assert self.momentum <= 1 and self.order > 0 and self.delta > 0\n",
    "        self.register_buffer(\"running_delta\", torch.zeros(1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.momentum > 0:\n",
    "            self.running_delta.fill_(self.delta * 0.7979)\n",
    "        else:\n",
    "            self.running_delta.fill_(self.delta)\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        return ternary(input_f, self.running_delta, self.delta, self.momentum,\n",
    "                       self.training and self.track_running_stats, self.order, self.use_scale)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \", \".join([\"{}={}\".format(k, v) for k, v in self.config.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12712/4249798583.py:12: RuntimeWarning: invalid value encountered in log\n",
      "  x[:,i] = np.where(x[:,i] > 0, np.log(x[:,i]), 0)\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([df_unsw_normal, df_bot_dos, df_bot_ddos], ignore_index=True)\n",
    "\n",
    "x = df_combined.loc[:, df_combined.columns != 'category']\n",
    "# x = (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "df_combined.category = pd.factorize(df_combined.category)[0]\n",
    "y = df_combined['category']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "for i in [0,1,2,3,7,8]:\n",
    "    x[:,i] = np.where(x[:,i] > 0, np.log(x[:,i]), 0)\n",
    "    \n",
    "x_label_data = torch.tensor(x, dtype=torch.float).to(device)\n",
    "y_label_data = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "dataset = TensorDataset(x_label_data, y_label_data)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "valid_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8192\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryActivationFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Binarize input to -1 or 1\n",
    "        output = input.sign()\n",
    "        output[output==0] = 1\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.abs() > 1] = 0\n",
    "        return grad_input\n",
    "\n",
    "class BinaryActivation(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return BinaryActivationFunction.apply(input)\n",
    "\n",
    "class QLinear(torch.nn.Linear):\n",
    "    qa_config = {}\n",
    "    qw_config = {}\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        # self.input_quantizer = BinaryActivation()\n",
    "        self.weight_quantizer = BinaryActivation()\n",
    "\n",
    "    def forward(self, input_f):\n",
    "        # input_t = self.input_quantizer(input_f)\n",
    "        weight_b = self.weight_quantizer(self.weight)\n",
    "        out = F.linear(input_f, weight_b, self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Validation Accuracy: 75.11%4\n",
      "Epoch [2/5], Validation Accuracy: 92.33%3\n",
      "Epoch [3/5], Validation Accuracy: 92.15%4\n",
      "Epoch [4/5], Validation Accuracy: 89.12%4\n",
      "Epoch [5/5], Validation Accuracy: 92.27%6\n",
      "Test Accuracy: 92.16%\n"
     ]
    }
   ],
   "source": [
    "class Netb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netb, self).__init__()\n",
    "        self.input_quantizer = BinaryActivation()\n",
    "\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.i1 = nn.Identity()\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        # self.i2 = nn.Identity()\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        # self.i3 = nn.Identity()\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.i2(x)\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.i3(x)\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_b = Netb().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_b.parameters(), 0.01,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model_b.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_b(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model_ter1.set_weights_and_biases()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    model_b.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_b(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final evaluation on the test set\n",
    "model_b.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_b(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model_b.state_dict(), 'saved_best_model_bnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
       "         -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
       "          1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
       "         -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
       "          1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
       "          1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "         -1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "         -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
       "         -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
       "          1., -1.],\n",
       "        [ 1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
       "          1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
       "          1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
       "         -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "         -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
       "          1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "          1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
       "          1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "          1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "          1.,  1.],\n",
       "        [ 1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
       "         -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
       "          1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "         -1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "          1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "         -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
       "          1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
       "          1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
       "         -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "         -1.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryActivationFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Binarize input to -1 or 1\n",
    "        output = input.sign()\n",
    "        output[output==0] = 1\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.abs() > 1] = 0\n",
    "        return grad_input\n",
    "\n",
    "class BinaryActivation(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return BinaryActivationFunction.apply(input)\n",
    "\n",
    "class QLinear(torch.nn.Linear):\n",
    "    qa_config = {}\n",
    "    qw_config = {}\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.weight_quantizer = BinaryActivation()\n",
    "    def forward(self, input_f):\n",
    "        weight_b = self.weight_quantizer(self.weight)\n",
    "        out = F.linear(input_f, weight_b, self.bias)\n",
    "        return out\n",
    "\n",
    "class Netb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netb, self).__init__()\n",
    "        self.input_quantizer = BinaryActivation()\n",
    "        self.linear1 = QLinear(13, 128, bias = False)\n",
    "\n",
    "        self.linear2 = QLinear(128, 128, bias = False)\n",
    "\n",
    "        self.linear3 = QLinear(128, 128, bias = False)\n",
    "\n",
    "        self.linear4 = QLinear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        x = self.input_quantizer(x)\n",
    "        x = self.linear4(x)\n",
    "        # x = self.act4(x)\n",
    "        return x\n",
    "\n",
    "model_bnn = Netb().to(device)\n",
    "model_bnn.linear1.state_dict()['weight'].copy_(model_b.linear1.state_dict()['weight'].sign())\n",
    "model_bnn.linear2.state_dict()['weight'].copy_(model_b.linear2.state_dict()['weight'].sign())\n",
    "model_bnn.linear3.state_dict()['weight'].copy_(model_b.linear3.state_dict()['weight'].sign())\n",
    "model_bnn.linear4.state_dict()['weight'].copy_(model_b.linear4.state_dict()['weight'].sign())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.16%\n"
     ]
    }
   ],
   "source": [
    "model_bnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_bnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "torch.save(model_bnn.state_dict(), 'saved_best_model_bnn_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_concatenate(arr):\n",
    "    # Chuyển đổi -1 thành 1 và 1 thành 0\n",
    "    arr = torch.flip(arr, dims=[0])\n",
    "    converted_arr = [1 if x == -1 else 0 for x in arr]\n",
    "    # Ghép các phần tử lại thành một chuỗi\n",
    "    concatenated_str = ''.join(map(str, converted_arr))\n",
    "\n",
    "    current_length = len(concatenated_str)\n",
    "    total_length = (np.ceil(current_length/32)*32).astype(int)\n",
    "    if current_length < total_length:\n",
    "        padding_length = int(total_length - current_length)\n",
    "        concatenated_str = '0' * padding_length + concatenated_str\n",
    "    # Chuyển chuỗi thành số nhị phân và sau đó thành số hex\n",
    "    hex_value = hex(int(concatenated_str, 2))[2:]  # Bỏ tiền tố '0x'\n",
    "\n",
    "    # Đảm bảo chuỗi hex đủ độ dài cần thiết\n",
    "    hex_length = total_length // 4  # 1 hex digit = 4 bits\n",
    "    if len(hex_value) < hex_length:\n",
    "        hex_value = '0' * (hex_length - len(hex_value)) + hex_value\n",
    "    return hex_value\n",
    "\n",
    "def save_model_parameters_to_txt(model, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            f.write(f'{name}\\n')\n",
    "            f.write(f'input_channel: {param.shape[1]}\\n')\n",
    "            f.write(f'output_channel: {param.shape[0]}\\n')\n",
    "            for param_e in param:\n",
    "                hex_conv = convert_and_concatenate(param_e)\n",
    "                segments = [hex_conv[i:i+8] for i in range(0, len(hex_conv), 8)]\n",
    "                segments.reverse()\n",
    "                for segment in segments:\n",
    "                    f.write(f'0x{segment}\\n')\n",
    "            f.write(f'\\n')\n",
    "    return 0\n",
    "\n",
    "# Gọi hàm để lưu thông số mô hình\n",
    "a = save_model_parameters_to_txt(model_bnn, 'model_parameters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.12%\n"
     ]
    }
   ],
   "source": [
    "model_bnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_bnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Netb(\n",
       "  (input_quantizer): BinaryActivation()\n",
       "  (linear1): QLinear(\n",
       "    in_features=13, out_features=128, bias=False\n",
       "    (weight_quantizer): BinaryActivation()\n",
       "  )\n",
       "  (linear2): QLinear(\n",
       "    in_features=128, out_features=128, bias=False\n",
       "    (weight_quantizer): BinaryActivation()\n",
       "  )\n",
       "  (linear3): QLinear(\n",
       "    in_features=128, out_features=128, bias=False\n",
       "    (weight_quantizer): BinaryActivation()\n",
       "  )\n",
       "  (linear4): QLinear(\n",
       "    in_features=128, out_features=3, bias=False\n",
       "    (weight_quantizer): BinaryActivation()\n",
       "  )\n",
       "  (act4): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bnn.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    outputs = model_bnn(images.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Validation Accuracy: 91.52%1\n",
      "Epoch [2/2], Validation Accuracy: 91.63%0\n",
      "Test Accuracy: 91.51%\n"
     ]
    }
   ],
   "source": [
    "class Net_fp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_fp, self).__init__()\n",
    "        self.linear1 = nn.Linear(13, 128, bias = False)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear2 = nn.Linear(128, 128, bias = False)\n",
    "        # self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.linear3 = nn.Linear(128, 128, bias = False)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.linear4 = nn.Linear(128, 3, bias = False)\n",
    "        self.act4 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        # x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.act4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_fp = Net_fp().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_fp.parameters(), 0.01,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4)\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model_fp.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_fp(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model_ter1.set_weights_and_biases()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    model_fp.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_fp(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final evaluation on the test set\n",
    "model_fp.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_fp(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model_fp.state_dict(), 'saved_best_model_fp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_fp_parameters_to_txt(model, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for name, param in model.named_parameters():\n",
    "            f.write(f'{name}\\n')\n",
    "            if('linear' in name and 'weight'in name):\n",
    "                f.write(f'input_channel: {param.shape[1]}\\n')\n",
    "                f.write(f'output_channel: {param.shape[0]}\\n')\n",
    "            f.write(f'{param}\\n')\n",
    "            f.write(f'\\n')\n",
    "    return 0\n",
    "\n",
    "# Gọi hàm để lưu thông số mô hình\n",
    "a = save_model_fp_parameters_to_txt(model_fp, 'fp_model_parameters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([0.9876, 0.9864, 0.9845, 0.9805, 1.0014, 0.9878, 0.9857, 0.9918, 0.9910,\n",
       "                      0.9740, 0.9809, 0.9821, 0.9844, 0.9829, 0.9852, 0.9847, 0.9843, 0.9849,\n",
       "                      0.9867, 0.9938, 0.9877, 0.9874, 0.9955, 0.9818, 0.9804, 0.9836, 0.9967,\n",
       "                      0.9887, 0.9758, 0.9847, 0.9884, 0.9849, 0.9843, 0.9887, 0.9859, 1.0009,\n",
       "                      0.9802, 1.0014, 0.9846, 0.9846, 0.9868, 0.9878, 0.9842, 1.0002, 0.9851,\n",
       "                      0.9809, 0.9777, 0.9897, 0.9965, 0.9884, 0.9831, 0.9852, 0.9786, 0.9919,\n",
       "                      0.9769, 0.9830, 0.9811, 0.9776, 0.9879, 0.9862, 0.9808, 0.9792, 0.9844,\n",
       "                      0.9930, 0.9875, 0.9875, 0.9828, 0.9937, 0.9916, 0.9778, 0.9867, 0.9819,\n",
       "                      0.9794, 0.9804, 0.9839, 0.9921, 0.9869, 0.9823, 0.9920, 0.9908, 0.9864,\n",
       "                      0.9843, 0.9846, 0.9862, 0.9877, 0.9890, 0.9839, 0.9883, 0.9829, 0.9843,\n",
       "                      0.9796, 0.9813, 0.9953, 0.9835, 0.9830, 0.9828, 0.9796, 0.9818, 0.9862,\n",
       "                      0.9825, 0.9803, 0.9932, 0.9843, 0.9778, 0.9823, 0.9835, 0.9851, 0.9991,\n",
       "                      0.9888, 0.9864, 0.9891, 0.9952, 0.9878, 0.9747, 0.9860, 0.9887, 0.9779,\n",
       "                      0.9875, 0.9876, 0.9731, 0.9852, 0.9901, 0.9873, 0.9847, 0.9874, 0.9868,\n",
       "                      0.9864, 0.9774], device='cuda:0')),\n",
       "             ('bias',\n",
       "              tensor([-4.4535e-03,  6.2985e-03, -1.2686e-03,  1.5138e-02, -2.4242e-03,\n",
       "                       9.3625e-03, -1.4216e-02,  1.2801e-02,  2.3946e-02,  1.0825e-03,\n",
       "                       7.4270e-03,  2.5004e-03, -1.0560e-03, -8.5635e-03, -2.3663e-03,\n",
       "                      -1.1285e-03,  9.3680e-03, -3.6514e-03,  5.2132e-03, -2.3449e-03,\n",
       "                      -4.4218e-03, -8.4914e-03,  3.9172e-03,  3.1120e-03, -8.0490e-04,\n",
       "                      -2.9338e-03,  4.2871e-03,  8.2725e-03, -5.6588e-03, -6.3861e-03,\n",
       "                      -2.2437e-03,  1.0238e-02, -3.0719e-03,  6.9304e-03, -1.1386e-02,\n",
       "                      -1.5159e-02, -4.6135e-03,  1.6230e-02, -5.8698e-04,  1.4021e-02,\n",
       "                      -2.8619e-03, -3.9335e-04, -3.5969e-03, -4.1907e-04,  6.3349e-03,\n",
       "                       6.4607e-04, -7.7578e-03, -1.1094e-03, -1.7940e-02, -8.9194e-04,\n",
       "                      -1.1472e-03,  7.8213e-03,  7.2425e-03, -1.2768e-02, -2.3749e-03,\n",
       "                      -6.3937e-03,  7.7080e-04, -2.5121e-03, -2.8414e-03, -4.6533e-03,\n",
       "                       2.2176e-03, -1.1373e-03,  1.1413e-03,  8.1605e-03,  6.3279e-03,\n",
       "                      -6.1149e-03,  4.1847e-03,  3.8230e-03, -1.8121e-04,  5.2244e-03,\n",
       "                       1.3307e-03,  2.9978e-03, -3.0094e-03,  3.1320e-03,  1.2696e-02,\n",
       "                       3.9094e-03, -4.8467e-03, -7.7091e-03, -1.1235e-02,  5.6691e-03,\n",
       "                       5.9213e-03,  4.9708e-03,  2.7314e-03, -1.3482e-02, -1.2322e-03,\n",
       "                      -1.6370e-03, -1.2956e-03, -3.6819e-03,  4.6449e-03, -7.7243e-03,\n",
       "                      -4.1553e-03, -4.8737e-03,  1.3976e-02, -6.4301e-03, -6.3901e-03,\n",
       "                       1.8567e-02, -6.3216e-03, -4.4773e-03,  4.8819e-03,  5.4418e-04,\n",
       "                       1.7031e-03, -1.0379e-02, -9.3371e-05, -1.0075e-03, -5.7866e-03,\n",
       "                      -3.7571e-05, -6.1946e-06,  4.5542e-03,  2.1419e-02, -3.6083e-03,\n",
       "                       7.5206e-03, -3.4943e-03,  9.0605e-04, -1.3774e-02,  1.2895e-04,\n",
       "                       1.3302e-02, -4.0796e-03, -2.7368e-03, -8.8684e-04, -3.4172e-03,\n",
       "                      -6.0653e-03, -3.4784e-03, -1.5941e-03,  4.5090e-03,  3.6928e-03,\n",
       "                      -1.3618e-02,  3.2160e-03, -7.7499e-03], device='cuda:0')),\n",
       "             ('running_mean',\n",
       "              tensor([-0.6664,  0.3375, -0.0014, -0.1311, -0.0199, -0.0727, -0.1342,  0.2918,\n",
       "                      -0.8623, -0.2934, -0.2139, -0.4570,  0.5402,  0.3458,  0.4330, -0.2258,\n",
       "                      -0.4373, -0.1937,  0.3421,  0.3950,  0.0489,  0.8011, -0.2329,  0.5464,\n",
       "                      -0.8919,  0.0993,  0.0197,  0.4189,  0.3331,  0.1253, -0.2152,  0.2982,\n",
       "                       0.3272,  0.5984,  0.4694,  0.4743, -0.7712,  0.7137, -0.1687, -0.2970,\n",
       "                      -0.1619, -0.3825, -0.1054,  0.2325,  0.0409,  0.1459,  0.0359, -0.3505,\n",
       "                       0.1511,  0.0073, -0.7033, -0.2397, -0.4111, -0.4374, -0.2324, -0.2179,\n",
       "                       0.2273, -0.2443,  0.2628,  0.0976, -0.5860, -0.1532, -0.8383,  0.1957,\n",
       "                      -0.2975, -0.1072, -0.4451,  0.2047,  0.1127, -0.2166, -0.1050,  0.4060,\n",
       "                       0.1218, -0.4408, -0.6814, -0.2814, -0.7712, -0.0976, -0.1317, -0.7580,\n",
       "                      -0.1859,  0.0341, -0.3948,  0.4865, -0.0427, -0.3968, -0.1957,  0.5937,\n",
       "                       0.0528, -0.0891,  0.2758, -0.1355,  0.4339, -0.0981, -0.1082,  0.5354,\n",
       "                       0.1880, -0.2915, -0.0349, -0.3455,  0.1447,  0.1249, -0.7994, -0.6096,\n",
       "                      -0.1041,  0.4961, -0.2220, -0.4956, -0.0760,  0.2628,  0.0305,  0.3587,\n",
       "                       0.0084,  0.1538, -0.1470,  0.0198,  0.2602,  0.0702,  0.7399,  0.1782,\n",
       "                       0.4360, -0.4455,  0.3825, -0.2894,  0.0667,  0.1712,  0.4221, -0.3216],\n",
       "                     device='cuda:0')),\n",
       "             ('running_var',\n",
       "              tensor([0.2041, 0.1753, 0.4285, 0.2553, 0.2533, 0.1339, 0.2005, 0.1401, 0.2245,\n",
       "                      0.1636, 0.2468, 0.2695, 0.2418, 0.2820, 0.2589, 0.1701, 0.4614, 0.0644,\n",
       "                      0.4246, 0.1313, 0.3789, 0.2534, 0.2085, 0.1862, 0.1281, 0.3688, 0.1801,\n",
       "                      0.6189, 0.2013, 0.3872, 0.7408, 0.2866, 0.1855, 0.2313, 0.2571, 0.1612,\n",
       "                      0.1263, 0.2515, 0.2528, 0.3597, 0.5748, 0.3477, 0.2156, 0.2957, 0.0499,\n",
       "                      0.2188, 0.2164, 0.1983, 0.1202, 0.2489, 0.0860, 0.1513, 0.2145, 0.2440,\n",
       "                      0.1701, 0.3203, 0.3664, 0.1698, 0.1868, 0.2353, 0.3936, 0.1039, 0.3135,\n",
       "                      0.1926, 0.7998, 0.2256, 0.2465, 0.6155, 0.2864, 0.1810, 0.2089, 0.1135,\n",
       "                      0.5914, 0.1905, 0.1625, 0.1728, 0.4838, 0.1361, 0.4764, 0.1417, 0.1082,\n",
       "                      0.1919, 0.5004, 0.3420, 0.2141, 0.1755, 0.4691, 0.0280, 0.1370, 0.2435,\n",
       "                      0.1569, 0.2040, 0.3004, 0.2197, 0.2955, 0.1763, 0.3133, 0.3376, 0.2541,\n",
       "                      0.2546, 0.1615, 0.2678, 0.2264, 0.1800, 0.1728, 0.1378, 0.5904, 0.1904,\n",
       "                      0.1687, 0.1752, 0.1028, 0.1665, 0.8487, 0.2262, 0.5209, 0.4316, 0.0625,\n",
       "                      0.1013, 0.2013, 0.1209, 0.2649, 0.1451, 0.1542, 0.1078, 0.2649, 0.3002,\n",
       "                      0.1323, 0.1155], device='cuda:0')),\n",
       "             ('num_batches_tracked', tensor(1440, device='cuda:0'))])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp.bn1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "def bit_count(n):\n",
    "    n = (n & 0x55555555) + ((n >> 1) & 0x55555555)\n",
    "    n = (n & 0x33333333) + ((n >> 2) & 0x33333333)\n",
    "    n = (n & 0x0f0f0f0f) + ((n >> 4) & 0x0f0f0f0f)\n",
    "    n = (n & 0x00ff00ff) + ((n >> 8) & 0x00ff00ff)\n",
    "    n = (n & 0x0000ffff) + ((n >> 16) & 0x0000ffff)\n",
    "    return n\n",
    "\n",
    "# Ví dụ sử dụng:\n",
    "n = 0b11111111101111000011111111111100  # 13 trong hệ thập phân\n",
    "print(bit_count(n))  # Kết quả sẽ là 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human_action",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
